{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cd4ee4dd",
      "metadata": {
        "id": "cd4ee4dd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91eeece9",
      "metadata": {
        "id": "91eeece9",
        "outputId": "fbee58e7-498d-4834-fa11-d721e8e3436d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\iq\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\iq\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import csv # csv reader\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
        "import numpy as np\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "stopword_set = set(nltk.corpus.stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10297c23",
      "metadata": {
        "id": "10297c23"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    \"\"\"Load data from a tab-separated file and append it to raw_data.\"\"\"\n",
        "    with open(path,encoding=\"utf8\") as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        index = 0 \n",
        "        for line in reader:\n",
        "            if line[0] == \"Id\":  # skip header\n",
        "                continue\n",
        "            #print(index)\n",
        "            (label, text) = parse_data_line(line)\n",
        "            raw_data.append((text, label))\n",
        "            index +=1\n",
        "\n",
        "def split_and_preprocess_data(percentage):\n",
        "    \"\"\"Split the data between train_data and test_data according to the percentage\n",
        "    and performs the preprocessing.\"\"\"\n",
        "    num_samples = len(raw_data)\n",
        "    num_training_samples = int((percentage * num_samples))\n",
        "    train_index = 0\n",
        "    \n",
        "    for (text, label) in raw_data[:num_training_samples]:\n",
        "        train_data.append((to_feature_vector(pre_process(text)),label))\n",
        "\n",
        "      \n",
        "    for (text, label) in raw_data[num_training_samples:]:    \n",
        "        test_data.append((to_feature_vector(pre_process(text)),label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "715adba0",
      "metadata": {
        "id": "715adba0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12846d2b",
      "metadata": {
        "id": "12846d2b"
      },
      "outputs": [],
      "source": [
        "def convert_label(label):\n",
        "    \"\"\"Converts the multiple classes into two,\n",
        "    making it a binary distinction between fake news and real.\"\"\"\n",
        "    #return label\n",
        "    # Converting the multiclass labels to binary label\n",
        "    labels_map = {\n",
        "        'true': 'REAL',\n",
        "        'mostly-true': 'REAL',\n",
        "        'half-true': 'REAL',\n",
        "        'false': 'FAKE',\n",
        "        'barely-true': 'FAKE',\n",
        "        'pants-fire': 'FAKE'\n",
        "    }\n",
        "    return labels_map[label]\n",
        "\n",
        "\n",
        "def parse_data_line(line):  \n",
        "    # Should return a tuple of the label as just FAKE or REAL and the statement\n",
        "    # e.g. (label, statement)\n",
        "    \"\"\" \"\"\"\n",
        "    (label,text) = line[1],line[2]\n",
        "    label = convert_label(label)\n",
        "    return (label,text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08f2e7c7",
      "metadata": {
        "id": "08f2e7c7"
      },
      "outputs": [],
      "source": [
        "def pre_process(text):\n",
        "    # Should return a list of tokens\n",
        "    # DESCRIBE YOUR METHOD IN WORDS\n",
        "    text_tokens = text.split()\n",
        "    output = []\n",
        "    for word in text_tokens :\n",
        "        if word:\n",
        "              output.append(word)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18b71bfa",
      "metadata": {
        "id": "18b71bfa"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "global_feature_dict = {} # A global dictionary of features\n",
        "from collections import Counter\n",
        "from nltk import ngrams\n",
        "import math\n",
        "\n",
        "def to_feature_vector(tokens):\n",
        "    # Should return a dictionary containing features as keys, and weights as values\n",
        "    # DESCRIBE YOUR METHOD IN WORDS\n",
        "    tokens = [word.lower() for word in tokens ]\n",
        "    dic_vector = dict(Counter(tokens))\n",
        "\n",
        "    result = dic_vector \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6958c032",
      "metadata": {
        "id": "6958c032"
      },
      "outputs": [],
      "source": [
        "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
        "\n",
        "def train_classifier(data):\n",
        "    print(\"Training Classifier...\")\n",
        "    pipeline =  Pipeline([\n",
        "                           ('svc', LinearSVC(max_iter=1000,C=1.3))])\n",
        "    return SklearnClassifier(pipeline).train(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6615f46",
      "metadata": {
        "id": "c6615f46"
      },
      "source": [
        "# Question 3: Cross-validation (20 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a553ca54",
      "metadata": {
        "id": "a553ca54"
      },
      "outputs": [],
      "source": [
        "#solution\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def cross_validate(dataset, folds):\n",
        "    results = []\n",
        "    fold_size = int(len(dataset)/folds) + 1\n",
        "    per = []\n",
        "    recall = []\n",
        "    f1score = []\n",
        "    accu = []\n",
        "    \n",
        "    for i in range(0,len(dataset),int(fold_size)):\n",
        "        # insert code here that trains and tests on the 10 folds of data in the dataset\n",
        "        print(\"Fold start on items %d - %d\" % (i, i+fold_size))\n",
        "        # FILL IN THE METHOD HERE\n",
        "        test_set = dataset[i:i+fold_size]\n",
        "        train_set = dataset[:i]+dataset[i+fold_size:]\n",
        "        \n",
        "        classifier = train_classifier(train_set)\n",
        "        \n",
        "        true_label = [t[1] for t in test_set]#ground truth\n",
        "        y_test = predict_labels([x[0] for x in test_set],classifier)\n",
        "        \n",
        "        \n",
        "        \n",
        "        output = classification_report(true_label,y_test,output_dict=True)\n",
        "        accuracy = accuracy_score(true_label, y_test)\n",
        "        \n",
        "        accu.append(accuracy)\n",
        "        per.append(output['FAKE']['precision'])\n",
        "        recall.append(output['FAKE']['recall'])\n",
        "        f1score.append(output['FAKE']['f1-score'])\n",
        "        \n",
        "        \n",
        "    return [np.mean(per),np.mean(recall),np.mean(f1score),np.mean(accu)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6b9c32e",
      "metadata": {
        "id": "c6b9c32e"
      },
      "outputs": [],
      "source": [
        "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
        "\n",
        "def predict_labels(samples, classifier):\n",
        "    \"\"\"Assuming preprocessed samples, return their predicted labels from the classifier model.\"\"\"\n",
        "    return classifier.classify_many(samples)\n",
        "\n",
        "def predict_label_from_raw(sample, classifier):\n",
        "    \"\"\"Assuming raw text, return its predicted label from the classifier model.\"\"\"\n",
        "    return classifier.classify(to_feature_vector(preProcess(reviewSample)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d488da1",
      "metadata": {
        "id": "9d488da1",
        "outputId": "2485fc8b-5802-4324-d64f-8829921c92fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now 0 rawData, 0 trainData, 0 testData\n",
            "Preparing the dataset...\n",
            "Now 10241 rawData, 0 trainData, 0 testData\n",
            "Preparing training and test data...\n",
            "I am before split_and_preprocess_data\n",
            "I am after split_and_preprocess_data\n",
            "After split, 10241 rawData, 8192 trainData, 2049 testData\n",
            "Training Samples: \n",
            "8192\n",
            "Features: \n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# MAIN\n",
        "\n",
        "# loading reviews\n",
        "# initialize global lists that will be appended to by the methods below\n",
        "raw_data = []          # the filtered data from the dataset file\n",
        "train_data = []        # the pre-processed training data as a percentage of the total dataset\n",
        "test_data = []         # the pre-processed test data as a percentage of the total dataset\n",
        "\n",
        "\n",
        "# references to the data files\n",
        "data_file_path = 'fake_news.tsv'\n",
        "\n",
        "# Do the actual stuff (i.e. call the functions we've made)\n",
        "# We parse the dataset and put it in a raw data list\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
        "      \"Preparing the dataset...\",sep='\\n')\n",
        "\n",
        "load_data(data_file_path) \n",
        "\n",
        "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
        "# You do the cross validation on the 80% (training data)\n",
        "# We print the number of training samples and the number of features before the split\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
        "      \"Preparing training and test data...\",sep='\\n')\n",
        "print('I am before split_and_preprocess_data')\n",
        "split_and_preprocess_data(0.8)\n",
        "\n",
        "print('I am after split_and_preprocess_data')\n",
        "# We print the number of training samples and the number of features after the split\n",
        "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
        "      \"Training Samples: \", len(train_data), \"Features: \", len(global_feature_dict), sep='\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aa72af5",
      "metadata": {
        "id": "4aa72af5",
        "outputId": "7d03b1e8-0bdd-4c1f-b51e-41429648488c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold start on items 0 - 820\n",
            "Training Classifier...\n",
            "Fold start on items 820 - 1640\n",
            "Training Classifier...\n",
            "Fold start on items 1640 - 2460\n",
            "Training Classifier...\n",
            "Fold start on items 2460 - 3280\n",
            "Training Classifier...\n",
            "Fold start on items 3280 - 4100\n",
            "Training Classifier...\n",
            "Fold start on items 4100 - 4920\n",
            "Training Classifier...\n",
            "Fold start on items 4920 - 5740\n",
            "Training Classifier...\n",
            "Fold start on items 5740 - 6560\n",
            "Training Classifier...\n",
            "Fold start on items 6560 - 7380\n",
            "Training Classifier...\n",
            "Fold start on items 7380 - 8200\n",
            "Training Classifier...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.5039986113960203,\n",
              " 0.5015893961483191,\n",
              " 0.5023835099418511,\n",
              " 0.5683779887059954]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cross_validate(train_data, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0100ae",
      "metadata": {
        "id": "4c0100ae"
      },
      "source": [
        "### Questions 5 (20%) and 6 (20%) (recommend starting a new notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d035f2d1",
      "metadata": {
        "id": "d035f2d1",
        "outputId": "67be3984-17fb-4670-fc04-505ef3c7c84c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({'the': 2, 'bush': 1, 'tax': 1, 'cuts': 1, 'helped': 1, 'to': 1, 'create': 1, 'a': 1, 'substantial': 1, 'part': 1, 'of': 1, 'deficit.': 1}, 'REAL')\n",
            "Training Classifier...\n",
            "Done training!\n",
            "Precision: 0.569793\n",
            "Recall: 0.569546\n",
            "F Score:0.569665\n",
            "Accuracy Acheived : 0.5695\n"
          ]
        }
      ],
      "source": [
        "# Finally, check the accuracy of your classifier by training on all the traning data\n",
        "# and testing on the test set\n",
        "# Will only work once all functions are complete\n",
        "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
        "if functions_complete:\n",
        "    print(test_data[0])   # have a look at the first test data instance\n",
        "    classifier = train_classifier(train_data)  # train the classifier\n",
        "    test_true = [t[1] for t in test_data]   # get the ground-truth labels from the data\n",
        "    test_pred = predict_labels([x[0] for x in test_data], classifier)  # classify the test data to get predicted labels\n",
        "    final_scores = precision_recall_fscore_support(test_true, test_pred, average='weighted') # evaluate\n",
        "    print(\"Done training!\")\n",
        "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % final_scores[:3])\n",
        "    print(f'Accuracy Acheived : {round(accuracy_score(test_true, test_pred),4)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cac5a6b",
      "metadata": {
        "id": "1cac5a6b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c10a50fd",
      "metadata": {
        "id": "c10a50fd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "742567d8",
      "metadata": {
        "id": "742567d8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}